{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import csr_matrix, hstack, lil_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('Sample data/train/behaviors.tsv', sep='\\t',header=None, index_col=0, usecols=[0, 1, 3, 4])\n",
    "interactions.columns = ['uID','history','impLog']\n",
    "\n",
    "interactions['impLog'] = interactions['impLog'].apply(lambda x: [(y.split('-')[0], int(y.split('-')[1])) for y in x.split(' ')])\n",
    "interactions['history'] = interactions['history'].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv('Sample data/train/news.tsv',sep='\\t',header=None, index_col=0)\n",
    "news.columns = [\"Category\", \"SubCategory\", \"Title\", \"Abstract\", \"URL\", \"Entities\", \"RelatedEntities\"]\n",
    "news.drop(columns=[\"URL\", \"Entities\", \"RelatedEntities\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf vectorization\n",
    "def vectorize(data):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    vectorizer.fit(data)\n",
    "    return vectorizer.transform(data)\n",
    "tfidf = vectorize(news['Title'])\n",
    "# one-hot encoding\n",
    "def one_hot(data):\n",
    "    enc = OneHotEncoder()\n",
    "    return enc.fit_transform(data)\n",
    "one_hot_category = one_hot(news[['Category']])\n",
    "one_hot_subcategory = one_hot(news[['SubCategory']])\n",
    "# combine tf-idf and one_hot_category and one_hot_subcategory\n",
    "news_vector = hstack([tfidf, one_hot_category, one_hot_subcategory])\n",
    "news_map = dict(zip(news.index, news_vector.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = interactions.copy()\n",
    "temp['impLog'] = temp['impLog'].apply(lambda x: [tup[0] for tup in x if tup[1] == 1])\n",
    "\n",
    "def concatenate_logs(history, impLog):\n",
    "    if history is np.NaN and history is np.NaN:\n",
    "        return []\n",
    "    elif history is np.NaN:\n",
    "        return impLog\n",
    "    elif history is np.NaN:\n",
    "        return history\n",
    "    else:\n",
    "        return history + impLog\n",
    "\n",
    "# create a userDict where key are all unique user IDs and value is a list of read newsID (all newsIDs in history + newsIds in impLog where label is 1)\n",
    "user_dict = {row['uID']: concatenate_logs(row['history'], row['impLog']) for _, row in temp.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_ids = {}\n",
    "# for idx, nID in enumerate(set(news.index)):\n",
    "#     news_ids[nID] = idx\n",
    "\n",
    "# user_ids = {}\n",
    "# for idx, uID in enumerate(set(interactions['uID'])):\n",
    "#     user_ids[uID] = idx\n",
    "\n",
    "# # create an embedding matrix of shape (n_unique_users, n_unique_news)\n",
    "# embed = lil_matrix((len(user_ids), len(news_ids))) \n",
    "# for u in user_dict:\n",
    "#     for n in user_dict[u]:\n",
    "#         embed[user_ids[u], news_ids[n]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "# embed_svd = svd.fit_transform(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label/sub-label user-hitory one-hot encoding\n",
    "\n",
    "def process_row(row, categories):\n",
    "    category_count = {category: 0 for category in categories}\n",
    "    for category in row:\n",
    "        if category in category_count:\n",
    "            category_count[category] += 1\n",
    "    category_count = {k: category_count[k] for k in sorted(category_count)}\n",
    "    return list(category_count.values())\n",
    "\n",
    "def labels_one_hot(category):\n",
    "    categories = news[category].unique()\n",
    "    interactions[category] = interactions['history'].apply(lambda x: [news.loc[nID][category] for nID in x] if x is not np.NaN else [])\n",
    "    return interactions[category].apply(lambda x: process_row(x, categories))\n",
    "\n",
    "interactions['category_hist_encoded'] = labels_one_hot('Category')\n",
    "interactions['subcategory_hist_encoded'] = labels_one_hot('SubCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_explode = interactions[['uID', 'impLog', 'category_hist_encoded', 'subcategory_hist_encoded']].explode('impLog')\n",
    "\n",
    "interactions_explode['nID'] = interactions_explode['impLog'].apply(lambda x: news_map[x[0]] if x[0] in news_map else [0] * len(list(news_map.values())[0]))\n",
    "interactions_explode['label'] = interactions_explode['impLog'].apply(lambda x: x[1])\n",
    "# interactions_explode['uID'] = interactions_explode['uID'].apply(lambda x: embed_svd[user_ids[x], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id one-hot encoding\n",
    "# encoder = OneHotEncoder()\n",
    "# user_encoded = encoder.fit_transform(interactions_explode[['uID']])\n",
    "\n",
    "# standardized user history \n",
    "scalar = MinMaxScaler()\n",
    "category_hist_encoded = scalar.fit_transform(interactions_explode['category_hist_encoded'].to_list())\n",
    "subcategory_hist_encoded = scalar.fit_transform(interactions_explode['subcategory_hist_encoded'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_encoded = csr_matrix(interactions_explode['nID'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_encoded = csr_matrix(interactions_explode['uID'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = hstack([category_hist_encoded, subcategory_hist_encoded, news_encoded, embed_encoded]).tocsr()\n",
    "X_train = hstack([category_hist_encoded, subcategory_hist_encoded, news_encoded]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = interactions_explode['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "from tqdm import trange\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "class FactorizationMachineClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Factorization Machine [1]_ using Stochastic Gradient Descent.\n",
    "    For binary classification only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_iter : int, default 10\n",
    "        Number of iterations to train the algorithm.\n",
    "\n",
    "    n_factors : int, default 10\n",
    "        Number/dimension of features' latent factors.\n",
    "\n",
    "    learning_rate : float, default 0.1\n",
    "        Learning rate for the gradient descent optimizer.\n",
    "\n",
    "    reg_coef : float, default 0.01\n",
    "        Regularization strength for weights/coefficients.\n",
    "\n",
    "    reg_factors : float, default 0.01\n",
    "        Regularization strength for features' latent factors.\n",
    "\n",
    "    random_state : int, default 1234\n",
    "        Seed for the randomly initialized features latent factors\n",
    "\n",
    "    verbose : bool, default True\n",
    "        Whether to print progress bar while training.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    intercept_ : double\n",
    "        Intercept term, w0 based on the original notations.\n",
    "\n",
    "    coef_ : 1d ndarray, shape [n_features,]\n",
    "        Coefficients, w based on the original notations.\n",
    "\n",
    "    feature_factors_ : 2d ndarray, shape [n_factors, n_features]\n",
    "        Latent factors for all features. v based on the original\n",
    "        notations. The learned factors can be viewed as the\n",
    "        embeddings for each features. If a pair of features tends\n",
    "        to co-occur often, then their embeddings should be\n",
    "        close/similar (in terms of cosine similarity) to each other.\n",
    "\n",
    "    history_ : list\n",
    "        Loss function's history at each iteration, useful\n",
    "        for evaluating whether the algorithm converged or not.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `S. Rendle Factorization Machines (2010)\n",
    "            <http://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf>`_ \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter = 10, n_factors = 10,\n",
    "                 learning_rate = 0.1, reg_coef = 0.01,\n",
    "                 reg_factors = 0.01, random_state = 1234, verbose = False):\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "        self.reg_coef = reg_coef\n",
    "        self.n_factors = n_factors\n",
    "        self.reg_factors = reg_factors\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the input data and label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
    "            Data in sparse matrix format.\n",
    "\n",
    "        y : 1d ndarray, shape [n_samples,]\n",
    "            Training data's corresponding label.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "        self.intercept_ = 0.0\n",
    "\n",
    "        # the factors are often initialized with a mean of 0 and standard deviation\n",
    "        # of 1 / sqrt(number of latent factor specified)\n",
    "        np.random.seed(self.random_state)\n",
    "        self.feature_factors_ = np.random.normal(\n",
    "            scale = 1 / np.sqrt(self.n_factors), size = (self.n_factors, n_features))\n",
    "        \n",
    "        # the gradient is implemented in a way that requires\n",
    "        # the negative class to be labeled as -1 instead of 0\n",
    "        y = y.copy().astype(np.int32)\n",
    "        y[y == 0] = -1\n",
    "\n",
    "        loop = range(self.n_iter)\n",
    "        if self.verbose:\n",
    "            loop = trange(self.n_iter)\n",
    "\n",
    "        self.history_ = []\n",
    "        for _ in loop:\n",
    "            loss = _sgd_update(X.data, X.indptr, X.indices,\n",
    "                               y, n_samples, n_features,\n",
    "                               self.intercept_, self.coef_,\n",
    "                               self.feature_factors_, self.n_factors,\n",
    "                               self.learning_rate, self.reg_coef, self.reg_factors)\n",
    "            self.history_.append(loss)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Probability estimates. The returned estimates for\n",
    "        all classes are ordered by the label of classes.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
    "            Data in sparse matrix format.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        proba : 2d ndarray, shape [n_samples, n_classes]\n",
    "            The probability of the sample for each class in the model.\n",
    "        \"\"\"\n",
    "        pred = self._predict(X)\n",
    "        pred_proba = 1.0 / (1.0 + np.exp(-pred))\n",
    "        proba = np.vstack((1 - pred_proba, pred_proba)).T\n",
    "        return proba\n",
    "\n",
    "    def _predict(self, X):\n",
    "        \"\"\"Similar to _predict_instance but vectorized for all samples\"\"\"\n",
    "        linear_output = X * self.coef_\n",
    "        v = self.feature_factors_.T\n",
    "        term = (X * v) ** 2 - (X.power(2) * (v ** 2))\n",
    "        factor_output = 0.5 * np.sum(term, axis = 1)\n",
    "        return self.intercept_ + linear_output + factor_output\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse csr_matrix, shape [n_samples, n_features]\n",
    "            Data in sparse matrix format.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Predicted class label per sample.\n",
    "        \"\"\"\n",
    "        pred_proba = self.predict_proba(X)[:, 1]\n",
    "        return pred_proba.round().astype(np.int32)\n",
    "\n",
    "\n",
    "@njit\n",
    "def _sgd_update(data, indptr, indices, y, n_samples, n_features,\n",
    "                w0, w, v, n_factors, learning_rate, reg_w, reg_v):\n",
    "    \"\"\"\n",
    "    Compute the loss of the current iteration and update\n",
    "    gradients accordingly.\n",
    "    \"\"\"\n",
    "    loss = 0.0\n",
    "    for i in range(n_samples):\n",
    "        pred, summed = _predict_instance(data, indptr, indices, w0, w, v, n_factors, i)\n",
    "        \n",
    "        # calculate loss and its gradient\n",
    "        loss += _log_loss(pred, y[i])\n",
    "        loss_gradient = -y[i] / (np.exp(y[i] * pred) + 1.0)\n",
    "    \n",
    "        # update bias/intercept term\n",
    "        w0 -= learning_rate * loss_gradient\n",
    "\n",
    "        # update weight\n",
    "        for index in range(indptr[i], indptr[i + 1]):\n",
    "            feature = indices[index]\n",
    "            w[feature] -= learning_rate * (loss_gradient * data[index] + 2 * reg_w * w[feature])\n",
    "\n",
    "        # update factor\n",
    "        for factor in range(n_factors):\n",
    "            for index in range(indptr[i], indptr[i + 1]):\n",
    "                feature = indices[index]\n",
    "                term = summed[factor] - v[factor, feature] * data[index]\n",
    "                v_gradient = loss_gradient * data[index] * term\n",
    "                v[factor, feature] -= learning_rate * (v_gradient + 2 * reg_v * v[factor, feature])\n",
    "    \n",
    "    loss /= n_samples\n",
    "    return loss\n",
    "\n",
    "\n",
    "@njit\n",
    "def _predict_instance(data, indptr, indices, w0, w, v, n_factors, i):\n",
    "    \"\"\"predicting a single instance\"\"\"\n",
    "    summed = np.zeros(n_factors)\n",
    "    summed_squared = np.zeros(n_factors)\n",
    "\n",
    "    # linear output w * x\n",
    "    pred = w0\n",
    "    for index in range(indptr[i], indptr[i + 1]):\n",
    "        feature = indices[index]\n",
    "        pred += w[feature] * data[index]\n",
    "\n",
    "    # factor output\n",
    "    for factor in range(n_factors):\n",
    "        for index in range(indptr[i], indptr[i + 1]):\n",
    "            feature = indices[index]\n",
    "            term = v[factor, feature] * data[index]\n",
    "            summed[factor] += term\n",
    "            summed_squared[factor] += term * term\n",
    "\n",
    "        pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
    "    \n",
    "    # summed is the independent term that can be re-used\n",
    "    # during the gradient update stage\n",
    "    return pred, summed\n",
    "\n",
    "\n",
    "@njit\n",
    "def _log_loss(pred, y):\n",
    "    \"\"\"\n",
    "    negative log likelihood of the\n",
    "    current prediction and label, y.\n",
    "    \"\"\"\n",
    "    return np.log(np.exp(-pred * y) + 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = FactorizationMachineClassifier(n_iter = 10, learning_rate = 0.01, n_factors=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:24<00:00, 32.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>FactorizationMachineClassifier(learning_rate=0.01, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FactorizationMachineClassifier</label><div class=\"sk-toggleable__content\"><pre>FactorizationMachineClassifier(learning_rate=0.01, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "FactorizationMachineClassifier(learning_rate=0.01, verbose=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.612640413285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = fm.predict(X_train)\n",
    "\n",
    "roc_auc_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
